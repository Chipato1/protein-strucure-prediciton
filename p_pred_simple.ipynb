{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4a5cde",
   "metadata": {},
   "source": [
    "# Basic Stuff\n",
    "#### Author: Carl Winkler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05a31e",
   "metadata": {},
   "source": [
    "## Baseslines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a217b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "d7e1704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_counts(fileName, absolutes = False):\n",
    "    # We always expect the order C, E, H\n",
    "    colums_pred = ['AminoAcid','Coil_p','Strand_p','Helix_p']\n",
    "    data = []\n",
    "    with open(fileName) as f:\n",
    "        lines = f.readlines() # list containing lines of file\n",
    "        i = 0\n",
    "        A_row = []\n",
    "\n",
    "        for line in lines:\n",
    "            elems = line.split('\\t')\n",
    "            if i == 0:\n",
    "                # Amino is first column\n",
    "                A_row.append(line[0])\n",
    "                #Count for Coil\n",
    "                A_row.append(int(elems[1]))\n",
    "            elif i == 1:\n",
    "                #Count for Strand\n",
    "                A_row.append(int(elems[1]))\n",
    "            elif i == 2:\n",
    "                #Count for Helix\n",
    "                A_row.append(int(elems[1]))\n",
    "                data.append(A_row) \n",
    "\n",
    "            i = i + 1\n",
    "            if i > 2:\n",
    "                i = 0\n",
    "                A_row = []\n",
    "\n",
    "    # Calculate the probabilites\n",
    "    if absolutes == False:\n",
    "        for elem in data:\n",
    "            total_Count = sum(elem[1:])\n",
    "            elem[1] = elem[1] / total_Count\n",
    "            elem[2] = elem[2] / total_Count\n",
    "            elem[3] = elem[3] / total_Count\n",
    "\n",
    "    # Work with pd data frame here\n",
    "    return pd.DataFrame(data, columns = colums_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "da0631b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AminoAcid</th>\n",
       "      <th>Coil_p</th>\n",
       "      <th>Strand_p</th>\n",
       "      <th>Helix_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>0.315058</td>\n",
       "      <td>0.169215</td>\n",
       "      <td>0.515727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>0.395618</td>\n",
       "      <td>0.292801</td>\n",
       "      <td>0.311581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>0.549460</td>\n",
       "      <td>0.125833</td>\n",
       "      <td>0.324708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>0.345957</td>\n",
       "      <td>0.155869</td>\n",
       "      <td>0.498174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>0.305693</td>\n",
       "      <td>0.320682</td>\n",
       "      <td>0.373625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>G</td>\n",
       "      <td>0.671437</td>\n",
       "      <td>0.146378</td>\n",
       "      <td>0.182185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H</td>\n",
       "      <td>0.446070</td>\n",
       "      <td>0.223546</td>\n",
       "      <td>0.330384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K</td>\n",
       "      <td>0.393129</td>\n",
       "      <td>0.178781</td>\n",
       "      <td>0.428090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L</td>\n",
       "      <td>0.269999</td>\n",
       "      <td>0.242968</td>\n",
       "      <td>0.487033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>0.329096</td>\n",
       "      <td>0.221852</td>\n",
       "      <td>0.449053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>N</td>\n",
       "      <td>0.565136</td>\n",
       "      <td>0.144082</td>\n",
       "      <td>0.290782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P</td>\n",
       "      <td>0.703087</td>\n",
       "      <td>0.102705</td>\n",
       "      <td>0.194208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Q</td>\n",
       "      <td>0.357808</td>\n",
       "      <td>0.172079</td>\n",
       "      <td>0.470113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R</td>\n",
       "      <td>0.353963</td>\n",
       "      <td>0.209285</td>\n",
       "      <td>0.436752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S</td>\n",
       "      <td>0.500843</td>\n",
       "      <td>0.192196</td>\n",
       "      <td>0.306961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T</td>\n",
       "      <td>0.435419</td>\n",
       "      <td>0.275831</td>\n",
       "      <td>0.288751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>V</td>\n",
       "      <td>0.254688</td>\n",
       "      <td>0.416232</td>\n",
       "      <td>0.329080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>W</td>\n",
       "      <td>0.303258</td>\n",
       "      <td>0.297486</td>\n",
       "      <td>0.399256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>X</td>\n",
       "      <td>0.241999</td>\n",
       "      <td>0.322103</td>\n",
       "      <td>0.435898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.307681</td>\n",
       "      <td>0.326530</td>\n",
       "      <td>0.365789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AminoAcid    Coil_p  Strand_p   Helix_p\n",
       "0          A  0.315058  0.169215  0.515727\n",
       "1          C  0.395618  0.292801  0.311581\n",
       "2          D  0.549460  0.125833  0.324708\n",
       "3          E  0.345957  0.155869  0.498174\n",
       "4          F  0.305693  0.320682  0.373625\n",
       "5          G  0.671437  0.146378  0.182185\n",
       "6          H  0.446070  0.223546  0.330384\n",
       "7          K  0.393129  0.178781  0.428090\n",
       "8          L  0.269999  0.242968  0.487033\n",
       "9          M  0.329096  0.221852  0.449053\n",
       "10         N  0.565136  0.144082  0.290782\n",
       "11         P  0.703087  0.102705  0.194208\n",
       "12         Q  0.357808  0.172079  0.470113\n",
       "13         R  0.353963  0.209285  0.436752\n",
       "14         S  0.500843  0.192196  0.306961\n",
       "15         T  0.435419  0.275831  0.288751\n",
       "16         V  0.254688  0.416232  0.329080\n",
       "17         W  0.303258  0.297486  0.399256\n",
       "18         X  0.241999  0.322103  0.435898\n",
       "19         Y  0.307681  0.326530  0.365789"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the different ACIDs we use for the last part\n",
    "parse_counts(\"testCounts.txt\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d480a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequences(fileName):\n",
    "    colums_pred = ['ID','length','Sequence','Target']\n",
    "    data = []\n",
    "    \n",
    "    with open(fileName) as f:\n",
    "        lines = f.readlines() # list containing lines of file\n",
    "        i = 0\n",
    "        A_row = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            elems = line.split('\\t')\n",
    "            if i == 0:\n",
    "                # ID\n",
    "                A_row.append(elems[0])\n",
    "            elif i == 1:\n",
    "                #length\n",
    "                A_row.append(int(elems[0]))\n",
    "            elif i == 2:\n",
    "                # Whole list of tokens - sequence\n",
    "                A_row.append(elems)\n",
    "            elif i == 3:\n",
    "                # Whole list of tokens - target\n",
    "                A_row.append(elems)\n",
    "                data.append(A_row) \n",
    "\n",
    "            #Logic for parsing every 4 things as a row\n",
    "            i = i + 1\n",
    "            if i > 3:\n",
    "                i = 0\n",
    "                A_row = []\n",
    "            \n",
    "    return pd.DataFrame(data, columns = colums_pred) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "1c51b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredDict(dataFrame):\n",
    "    dict_p = {}\n",
    "    dataFrame = dataFrame.reset_index()  # make sure indexes pair with number of rows\n",
    "    \n",
    "    for index, row in dataFrame.iterrows():\n",
    "        token = \"\"\n",
    "        if row['Coil_p'] > row['Helix_p'] and row['Coil_p'] > row['Strand_p']:\n",
    "            token = \"C\"\n",
    "        elif row['Strand_p'] > row['Helix_p']:\n",
    "            token = \"E\"\n",
    "        else:\n",
    "            token = \"H\"\n",
    "        dict_p[row['AminoAcid']] = token\n",
    "        \n",
    "    return dict_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c43e103c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naive approach maps the tokens as follows: {'A': 'H', 'C': 'C', 'D': 'C', 'E': 'H', 'F': 'H', 'G': 'C', 'H': 'C', 'K': 'H', 'L': 'H', 'M': 'H', 'N': 'C', 'P': 'C', 'Q': 'H', 'R': 'H', 'S': 'C', 'T': 'C', 'V': 'E', 'W': 'H', 'X': 'H', 'Y': 'H'}\n"
     ]
    }
   ],
   "source": [
    "df = parse_counts(\"trainCounts.txt\")\n",
    "print(\"The naive approach maps the tokens as follows:\", getPredDict(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2936d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct_pred(prediction, target_seq):\n",
    "    count = 0\n",
    "    for idx, tok in enumerate(prediction):\n",
    "        if tok == target_seq[idx]:\n",
    "            count = count + 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b15d0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts the secondary structure which the acid is seen in mostly\n",
    "class basic_naive_predict:\n",
    "    \n",
    "    def __init__(self, dict_p): \n",
    "        self.dict_p = dict_p\n",
    "    def get_name(self):\n",
    "        return \"Naive-Greedy\"\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        prediction = []\n",
    "        for token in sequence:\n",
    "            prediction.append(self.dict_p[token])\n",
    "        return prediction  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3fb7f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always predicts c\n",
    "class always_c_predict:\n",
    "        \n",
    "    def get_name(self):\n",
    "        return \"Always-C\"\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        prediction = []\n",
    "        for token in sequence:\n",
    "            prediction.append('C')\n",
    "        return prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1947cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Predicts randomly\n",
    "class random_predict:\n",
    "        \n",
    "    def get_name(self):\n",
    "        return \"Random-Token\"\n",
    "\n",
    "    def predict(self, sequence):\n",
    "        allowed_tok = [\"H\",\"E\",\"C\"]\n",
    "        prediction = []\n",
    "        for token in sequence:\n",
    "            prediction.append(random.choice(allowed_tok))\n",
    "        return prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f4c76713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_basic_predictor(predictor, filenames):\n",
    "    for filename in filenames:\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\"Evaluating predictior:\", predictor.get_name(), \" on: \", filename, \"\\n\")\n",
    "        \n",
    "        data_df = parse_sequences(filename)\n",
    "        \n",
    "        n_of_predicted_tokens = 0\n",
    "        n_of_correct_predictions = 0\n",
    "        \n",
    "        data_df = data_df.reset_index()\n",
    "        \n",
    "        for index, row in data_df.iterrows():\n",
    "            prediction = predictor.predict(row['Sequence'])\n",
    "            \n",
    "            n_of_predicted_tokens += len(prediction)\n",
    "            n_of_correct_predictions += number_of_correct_pred(prediction, row['Target'])\n",
    "\n",
    "        acc = n_of_correct_predictions / n_of_predicted_tokens\n",
    "        print(\"The accuracy is:\", acc, \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d77b3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_train_df = parse_counts(\"trainCounts.txt\")\n",
    "counts_test_df = parse_counts(\"testCounts.txt\")\n",
    "pred_dict_train = getPredDict(counts_train_df)\n",
    "pred_dict_test = getPredDict(counts_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5c911303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Naive-Greedy  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.49057580602822454 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Naive-Greedy  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.4893581866060825 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"trainCounts.txt\"\n",
    "predictor = basic_naive_predict(pred_dict_train)\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "69980164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Naive-Greedy  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.49057580602822454 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Naive-Greedy  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.4893581866060825 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "predictor = basic_naive_predict(pred_dict_test)\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62355ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5c89e7f",
   "metadata": {},
   "source": [
    "Now we evaluate the two other baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7521f863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Always-C  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.3984553719206006 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Always-C  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.39915113337119845 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate always_h_predict, It doesnt even need testCounts however its just a baseline so I didn't optimize that\n",
    "predictor = always_c_predict()\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d5e5dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Random-Token  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.33366033231835346 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Random-Token  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.3329219806888005 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate random_predict, It doesnt even need testCounts however its just a baseline so I didn't optimize that as well\n",
    "predictor = random_predict()\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f05ec8",
   "metadata": {},
   "source": [
    "## Using Windows :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c7644fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always predicts c\n",
    "class window_predict():\n",
    "    \n",
    "    def __init__(self, count_df, window_size, weights = -1): \n",
    "        self.count_df = count_df\n",
    "        #self.dict_p = dict_p\n",
    "        self.window_size = window_size\n",
    "        countdict = {}\n",
    "        \n",
    "        for index, row in self.count_df.iterrows():\n",
    "            #format C E H\n",
    "            countdict[row['AminoAcid']] = [row['Coil_p'],row['Strand_p'],row['Helix_p']]\n",
    "\n",
    "        self.dict_decision = countdict\n",
    "        \n",
    "        # Use uniform window if no window is given\n",
    "        if weights == -1:\n",
    "            self.weights = list = [1/window_size] * window_size \n",
    "        else:\n",
    "            self.weights = weights\n",
    "            \n",
    "    def get_name(self):\n",
    "        return \"Window-Predictor\"\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        prediction = []\n",
    "        for idx, token in enumerate(sequence):\n",
    "            # Check if we are on the edges and predict with window_size 1 then\n",
    "            \n",
    "            if idx < self.window_size/2 or idx > len(sequence)-self.window_size/2: \n",
    "                row = self.count_df.loc[self.count_df['AminoAcid'] == token].iloc[0]\n",
    "                token = \"\"\n",
    "                if row['Coil_p'] > row['Helix_p'] and row['Coil_p'] > row['Strand_p']:\n",
    "                    token = \"C\"\n",
    "                elif row['Strand_p'] > row['Helix_p']:\n",
    "                    token = \"E\"\n",
    "                else:\n",
    "                    token = \"H\"\n",
    "                prediction.append(token)\n",
    "                \n",
    "            else:\n",
    "                elem_in_window = sequence[idx - self.window_size // 2: idx + 1 + self.window_size // 2]\n",
    "                wind_probs = {'C': 0, 'E': 0, 'H': 0}\n",
    "                \n",
    "                # Iterate over window for each element\n",
    "                for wind_pos, weight in enumerate(self.weights):\n",
    "                    counts = self.dict_decision[elem_in_window[wind_pos]]\n",
    "                    wind_probs['C'] +=  counts[0] * weight\n",
    "                    wind_probs['E'] +=  counts[1] * weight\n",
    "                    wind_probs['H'] +=  counts[2] * weight\n",
    "                    \n",
    "                prediction.append(max(wind_probs, key=wind_probs.get))\n",
    "                #print(\"Elements in window: \", elem_in_window)\n",
    "                #print(\"Dict: \", wind_probs)\n",
    "                #print(\"Decision: \", max(wind_probs, key=wind_probs.get))\n",
    "                \n",
    "        return prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "888e4dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [71295, 38096, 118122], 'C': [15219, 11012, 11788], 'D': [91108, 20452, 55118], 'E': [67859, 30453, 99177], 'F': [35731, 36291, 44951], 'G': [131513, 29389, 36800], 'H': [29394, 14397, 22110], 'K': [66098, 29944, 72590], 'L': [73825, 66075, 135789], 'M': [15908, 10376, 22587], 'N': [70299, 17040, 36206], 'P': [90246, 12963, 25491], 'Q': [38836, 18797, 53058], 'R': [52829, 30235, 66170], 'S': [87485, 32805, 54994], 'T': [67458, 41901, 45222], 'V': [50802, 82384, 66285], 'W': [11710, 11523, 16033], 'X': [50033, 64830, 83208], 'Y': [31418, 32141, 37932]}\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5301336673369625 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5286301440594826 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=3)\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "cac6111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5372367926913126 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5358393142975164 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictor = window_predict(counts_train_df, window_size=5)\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "cc34756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5352453237931466 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5338720503950019 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictor = window_predict(counts_train_df, window_size=7)\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "593c7c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5276979930307257 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5258615170134765 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=3,weights = [0.25,0.5,0.25])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "d82668ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5107432863779435 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5096060308772654 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=3,weights = [0.4,0.1,0.4])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c3ecf906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5046371091486549 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5037001084318686 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=3,weights = [0.1,0.8,0.1])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7a462d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5204780080178902 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5192389115505758 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=3,weights = [0.2,0.6,0.2])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "b588df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5244033828963927 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5228089017400733 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=5,weights = [0.05,0.15,0.5,0.15,0.05])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "5d6f4a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5334542848136886 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5317757009345795 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=5,weights = [0.1,0.2,0.5,0.2,0.1])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "c8ac24c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5375724607904637 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5359601383797181 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=5,weights = [0.12,0.18,0.4,0.18,0.12])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e6336b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9799999999999999"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.12,0.18,0.4,0.18,0.12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "b11104c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.54233873973967 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5405504208189188 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=5,weights = [0.15,0.2,0.3,0.2,0.15])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "90eef59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5369458572210773 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5351030102752105 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=5,weights = [0.1,0.1,0.4,0.2,0.2])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "ab4a643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5420228372207375 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5401982754169463 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=5,weights = [0.15,0.15,0.3,0.2,0.2])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "5bb4872a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  trainSS.txt \n",
      "\n",
      "The accuracy is: 0.5071334672538085 \n",
      "\n",
      "-------------------------------------------------\n",
      "Evaluating predictior: Window-Predictor  on:  testSS.txt \n",
      "\n",
      "The accuracy is: 0.5061661589301389 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate naive approach based on \"testCounts.txt\"\n",
    "counts_train_df = parse_counts(\"trainCounts.txt\", True)\n",
    "predictor = window_predict(counts_train_df, window_size=7,weights = [0.5,0.1,0.2,0.4,0.2,0.1,0.5])\n",
    "evalute_basic_predictor(predictor, [\"trainSS.txt\", \"testSS.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75539c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cee10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Archive\n",
    "\n",
    "\n",
    "#Calculate the accuracy from the counts directly\n",
    "def acc_naive_fromCounts(fileName):\n",
    "    dataFrame = parse_counts(fileName)\n",
    "    dataFrame = dataFrame.reset_index()  # make sure indexes pair with number of rows\n",
    "    acc = 0\n",
    "    \n",
    "    for index, row in dataFrame.iterrows():\n",
    "        token = \"\"\n",
    "        if row['Coil_p'] > row['Helix_p'] and row['Coil_p'] > row['Strand_p']:\n",
    "            acc += row['Coil_p']\n",
    "        elif row['Strand_p'] > row['Helix_p']:\n",
    "            acc += row['Strand_p']\n",
    "        else:\n",
    "            acc += row['Helix_p']\n",
    "        \n",
    "    acc = acc / len(dataFrame['Coil_p'])  \n",
    "    return acc\n",
    "\n",
    "# Here we compare with the accuracies from the counts directly to see that the implementation is correct\n",
    "print(\"ACC with trainCounts:\", acc_naive_fromCounts(\"trainCounts.txt\"))\n",
    "print(\"ACC with testCounts:\", acc_naive_fromCounts(\"testCounts.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
